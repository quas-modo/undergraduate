{
    "version": "https://jsonfeed.org/version/1",
    "title": "Carpe diem",
    "subtitle": "pluck the day",
    "icon": "https://quas-modo.github.io/undergraduate/images/favicon.ico",
    "description": "notes/thoughts/nonsense",
    "home_page_url": "https://quas-modo.github.io/undergraduate",
    "items": [
        {
            "id": "https://quas-modo.github.io/undergraduate/2024/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%E5%90%84%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/",
            "url": "https://quas-modo.github.io/undergraduate/2024/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%E5%90%84%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/",
            "title": "快速了解各种神经网络",
            "date_published": "2024-04-04T06:09:20.000Z",
            "content_html": "<p>根据李沐 stanford 课程第三章梳理。</p>\n<p><span id=\"more\"></span></p>\n<h1 id=\"神经网络\"><a class=\"anchor\" href=\"#神经网络\">#</a> 神经网络</h1>\n<p><strong>神经网络是由具有适应性的简单单元组成的广泛并行互连的网络。</strong></p>\n<ul>\n<li>手工提取特征（用人的知识进行） --&gt;  神经网络来提取特征</li>\n<li>神经网络（可能更懂机器学习）来提取，可能对后面的线性或 softmax 回归可能会更好一些。</li>\n</ul>\n<p>用神经网络的好处在于，不用费心思去想，提取的数据特征是否会被模型喜欢，但是计算量和数量都比手工提取的数量级要大很多。</p>\n<h2 id=\"线性模型\"><a class=\"anchor\" href=\"#线性模型\">#</a> 线性模型</h2>\n<p>学习一条直线来拟合数据，适用于一些线性关系。</p>\n<p>使用均方误差（MSE）作为目标函数，使得预测值趋近于真实值，但是作为分类关心的是对应类别的置信度。</p>\n<p>解决方法：</p>\n<ul>\n<li>把预测的分数换成概率的形式（Softmax 函数）</li>\n<li>衡量真实值概率与预测值概率的区别，用 Cross-Entropy 交叉熵。只关心真实类比的预测情况。</li>\n</ul>\n<h2 id=\"多层感知机\"><a class=\"anchor\" href=\"#多层感知机\">#</a> 多层感知机</h2>\n<p><strong>稠密层</strong>（全连接层或线性层）有可学习的参数 W，b，计算 <strong>y=Wx+b</strong></p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240404141142675.png\" alt=\"image-20240404141142675\" /></p>\n<ul>\n<li>线性回归可以看成是，有 1 个输出的全连接层</li>\n<li>softmax 回归可以看成是，有 m 个输出加上 softmax 操作子</li>\n</ul>\n<p>加入非线性性：激活函数（如 sigmoid、relu 函数），这样做可以模拟各种非线性函数。</p>\n<p>可调整的超参数：隐藏层的输出维度、隐藏层的个数。</p>\n<h2 id=\"卷积神经网络\"><a class=\"anchor\" href=\"#卷积神经网络\">#</a> 卷积神经网络</h2>\n<p><strong>为什么要用 CNN</strong>：用 MLP 训练 ImageNet (300*300 的图片有 1000 个类)，MLP 有 1 个隐藏层，隐藏层的输出有 1 万（输入大概为 9 万输出是一千，随机取了个中间值 1 万），这样的话模型就有 10 亿个参数可供学习。 这样可学习的参数太多了。所以我们要用处理图片分类的先验信息：</p>\n<ul>\n<li>Translation invariance 平移不变性：需要识别的目标在一幅图的一个地方换到另一个地方，不会发生太多的变化</li>\n<li>locality 局部性：识别一个物体不需要看较远的像素</li>\n</ul>\n<p><strong>卷积核</strong>：k*k 的窗口，通常被训练用于识别图片的一个模式。</p>\n<p><strong>池化层（汇聚层）</strong>：卷积层对输入的位置是比较敏感的，任何一个物体在输入中移动时，会导致它对应的输出也会移动。对位置移动的鲁棒性，提出了 pooling layer（池化层或汇聚层）【就是每一次去计算 k*k 窗口的元素的<strong>均值</strong>（平均汇聚）或<strong>最大</strong>值（最大汇聚）】</p>\n<p>卷积神经网络就是一个神经网络，将卷积层堆起来，用卷积来抽取图片中的空间信息【不一定要做图片，可以做跟空间信息有关的东西，只要满足本地性和平移不变性】</p>\n<p><strong>神经网络的设计模式</strong>：更现代的 CNN，如 AlexNet，ResNet 等，考虑了不同结构化信息和关注的问题。</p>\n<h2 id=\"循环神经网络\"><a class=\"anchor\" href=\"#循环神经网络\">#</a> 循环神经网络</h2>\n<p><strong>Dense layer-&gt;Recurrent networks</strong>：</p>\n<ol>\n<li>MLP：预测分类（one-hot），由前一个词预测下一个词。但是没有办法很好的处理序列和时序信息。不能将前面的词相加，也不能改变输入的特征数量。</li>\n<li>RNN：将之前的信息（处理序列、时序信息）也传到这次的判断中。现代 RNN，对信息流有着更精细的控制，带有门。\n<ol>\n<li>LSTM：在一些情况下抑制 Xt，比如遇到空格或者停用词的时候</li>\n<li>GRU：在一些情况下抑制 ht-1，比如需要忘记前序信息的时候</li>\n</ol>\n</li>\n</ol>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240404141156000.png\" alt=\"image-20240404141156000\" /></p>\n<h3 id=\"bi-rnn和deep-rnn\"><a class=\"anchor\" href=\"#bi-rnn和deep-rnn\">#</a> Bi-RNN 和 Deep RNN</h3>\n<ul>\n<li>一个方向 --&gt; 两个方向：正向层：将过去时刻的信息放入当前时刻，然后 反向层：时刻 t+1 的信息往时刻 t 的方向走【P.S. 这里的时刻我觉得不是时间的概念要看成是文本中的词序可能会好理解一点】，最后结合正向层和反向层的信息相结合作为 yt</li>\n<li>把不同的层跟 MLP 一样累加起来，做成多层 RNN</li>\n</ul>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240404141207231.png\" alt=\"image-20240404141207231\" /></p>\n",
            "tags": [
                "nlp",
                "机器学习基础",
                "AI",
                "机器学习"
            ]
        },
        {
            "id": "https://quas-modo.github.io/undergraduate/2024/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%86%B3%E7%AD%96%E6%A0%91/",
            "url": "https://quas-modo.github.io/undergraduate/2024/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%86%B3%E7%AD%96%E6%A0%91/",
            "title": "决策树",
            "date_published": "2024-04-01T15:40:56.000Z",
            "content_html": "<p>根据西瓜书《决策树》章节和李沐 b 站相关视频整理。</p>\n<p><span id=\"more\"></span></p>\n<h1 id=\"决策树\"><a class=\"anchor\" href=\"#决策树\">#</a> 决策树</h1>\n<h2 id=\"西瓜书\"><a class=\"anchor\" href=\"#西瓜书\">#</a> 西瓜书</h2>\n<h3 id=\"基本流程\"><a class=\"anchor\" href=\"#基本流程\">#</a> 基本流程</h3>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234738414.png\" alt=\"image-20240401234738414\" /></p>\n<p>什么情况下不需要继续往下分了？三种停止递归条件如下：</p>\n<ol>\n<li>已全为同一种类别 -&gt; 叶节点</li>\n<li>已没有更多的属性往下分（西瓜色泽、敲声、根蒂，无其他属性）-&gt; 谁多当做谁</li>\n<li>遇到了未出现的属性值 -&gt; 父节点谁多当做谁</li>\n</ol>\n<p>最重要的是找到最优的划分属性</p>\n<blockquote>\n<p>名词解释</p>\n<p>先验分布：基于过去研究、专家经验等</p>\n<p>后验分布：先验分布 + 观测数据</p>\n</blockquote>\n<h3 id=\"划分选择\"><a class=\"anchor\" href=\"#划分选择\">#</a> 划分选择</h3>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234759543.png\" alt=\"image-20240401234759543\" /></p>\n<p>分散的类别越多、概率越小，信息熵越大</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234808225.png\" alt=\"image-20240401234808225\" /></p>\n<p>信息增益越大，意味着使用属性 a 来进行划分所获得的纯度提升越大。每一次选取能够获得最大信息增益的属性进行划分，直到不能划分为止。</p>\n<h4 id=\"增益率\"><a class=\"anchor\" href=\"#增益率\">#</a> 增益率</h4>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234827256.png\" alt=\"image-20240401234827256\" /></p>\n<p>C4.5 决策树算法</p>\n<p>增益越大越好，属性可选择值（分支数量）越小越好。</p>\n<blockquote>\n<p>规范化：把不可直接比较的东西，变得可以比较。</p>\n<p>归一化：规范化到 0-1 之间。</p>\n</blockquote>\n<h4 id=\"基尼指数\"><a class=\"anchor\" href=\"#基尼指数\">#</a> 基尼指数</h4>\n<p>CART 决策树学习算法</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234840889.png\" alt=\"image-20240401234840889\" /></p>\n<h3 id=\"剪枝处理\"><a class=\"anchor\" href=\"#剪枝处理\">#</a> 剪枝处理</h3>\n<p>剪枝是决策树对付 “过拟合” 的主要手段！</p>\n<blockquote>\n<p>过拟合：学习了数据集中不该学习的特性，学多了；不是一般规律，而是少量样本有的特性</p>\n</blockquote>\n<h4 id=\"预剪枝\"><a class=\"anchor\" href=\"#预剪枝\">#</a> 预剪枝</h4>\n<p>提前终止某些分支的生长。</p>\n<p>在生成树的时候就进行评估。如果划分后，精度增加，就划分；若精度无提升，就剪枝。</p>\n<p>但在有些情况下，后续划分可能带来性能显著提高，基于 “贪心” 的预剪枝可能会造成欠拟合。</p>\n<h4 id=\"后剪枝\"><a class=\"anchor\" href=\"#后剪枝\">#</a> 后剪枝</h4>\n<p>生成一颗完全树，再 “回头 “剪枝</p>\n<p>后剪枝可以减少欠拟合的风险，但是训练开销较大。</p>\n<h3 id=\"连续与缺失值\"><a class=\"anchor\" href=\"#连续与缺失值\">#</a> 连续与缺失值</h3>\n<h4 id=\"连续值处理\"><a class=\"anchor\" href=\"#连续值处理\">#</a> 连续值处理</h4>\n<p>最简单的处理策略是二分法。</p>\n<p>候选点集是连续属性之间的中位点，挑选增益率最高的二分点。</p>\n<h4 id=\"缺失值处理\"><a class=\"anchor\" href=\"#缺失值处理\">#</a> 缺失值处理</h4>\n<p>仅使用无缺失的样例，是对数据的极大浪费。</p>\n<p>那么如何使用缺失值？基本思路：<strong>样本赋权、权重划分</strong></p>\n<ol>\n<li>如何划分属性选择？</li>\n<li>给定划分属性，若样本在该属性上的值确实，如何进行划分？</li>\n</ol>\n<p>解决方案、思路：</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234853075.png\" alt=\"image-20240401234853075\" /></p>\n<p>回答问题一：如何划分属性选择？</p>\n<p>信息熵的计算增加权重，通过属性权重（非缺失样本概率）与子集的增益率，权衡选择。</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234907588.png\" alt=\"image-20240401234907588\" /></p>\n<p>回答问题二：给定划分属性，若样本在该属性上的值确实，如何进行划分？</p>\n<p>将已知的后验概率作为未知的先验概率进行处理，<strong>缺失值样本将同时进入三个分支</strong>，但在进入分支时，以权重为概率进行划分。</p>\n<h3 id=\"多变量决策树\"><a class=\"anchor\" href=\"#多变量决策树\">#</a> 多变量决策树</h3>\n<p>每个节点的决策不再依赖于单一属性，而是找到合适的线性分类器。</p>\n<h2 id=\"李沐\"><a class=\"anchor\" href=\"#李沐\">#</a> 李沐</h2>\n<p><strong>两个例子</strong>：</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234932374.png\" alt=\"image-20240401234932374\" /></p>\n<ul>\n<li>怎么用决策树来做分类：拿一个数据，看一下这个数据在节点上是应该往哪个方向走，一直往下走，走到叶子节点，就会得到数据所对应的标号</li>\n<li>用决策树做回归：与上面那个例子不同的是，叶节点不是一个具体的类而是一个实数值</li>\n</ul>\n<h3 id=\"决策树的好与坏\"><a class=\"anchor\" href=\"#决策树的好与坏\">#</a> <strong>决策树的好与坏</strong></h3>\n<p>好处：</p>\n<ul>\n<li>可以解释（可以让人看到对数据处理的过程）【常用于银行业保险业】；</li>\n<li>可以处理数值类和类别类的特征；</li>\n</ul>\n<p>坏处：</p>\n<ul>\n<li>不稳定（数据产生一定的噪音之后，整棵树构建出的样子可能会不一样）【使用集成学习 (ensemble learning) 可以解决】</li>\n<li>数据过于复杂会生成过于复杂的树，会导致过拟合【把决策树的枝剪掉一些（在训练时觉得太复杂了就停下来，或在训练之后把特往下的节点给剪掉）】</li>\n<li>大量的判断语句（太顺序化），不太好并行【在性能上会吃亏】</li>\n</ul>\n<h3 id=\"让决策树稳定的方法\"><a class=\"anchor\" href=\"#让决策树稳定的方法\">#</a> <strong>让决策树稳定的方法</strong></h3>\n<h4 id=\"随机森林\"><a class=\"anchor\" href=\"#随机森林\">#</a> 随机森林</h4>\n<ul>\n<li>训练多个决策树来提升稳定性：</li>\n<li>每棵树会独立的进行训练，训练之后这些树一起作用得出结果；</li>\n<li>分类的话，可以用投票（少数服从多数）；</li>\n<li>回归的话，实数值可以时每棵树的结果求平均；</li>\n<li>随机来自以下两种情况：\n<ul>\n<li>Bagging：在训练集中随机采样一些样本出来（放回，可重复）；</li>\n<li>在 bagging 出来的数字中，再随机采样一些特征出来，不用整个特征；</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"boosting\"><a class=\"anchor\" href=\"#boosting\">#</a> <strong>Boosting</strong></h4>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234948093.png\" alt=\"image-20240401234948093\" /></p>\n<ul>\n<li>顺序完成多个树的训练（之前是独立的完成）</li>\n<li>例子说的是，利用训练好的树与真实值做残差来训练新的树，训练好了之后再与之前的树相加</li>\n<li>残差 等价于 取了一个平均均方误差（预测值与真实值的）再求梯度乘上个负号</li>\n</ul>\n<h4 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> <strong>总结：</strong></h4>\n<p>​     树模型在工业界用的比较多【简单，训练算法简单，没有太多的超参数，结果还不错】（不用调参结果还不错）</p>\n<p>​      树模型能够用的时候，通常是第一选择。</p>\n",
            "tags": [
                "nlp",
                "机器学习基础",
                "AI",
                "机器学习"
            ]
        }
    ]
}