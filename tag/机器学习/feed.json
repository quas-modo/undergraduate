{
    "version": "https://jsonfeed.org/version/1",
    "title": "Carpe diem",
    "subtitle": "pluck the day",
    "icon": "https://quas-modo.github.io/images/favicon.ico",
    "description": "notes/thoughts/nonsense",
    "home_page_url": "https://quas-modo.github.io",
    "items": [
        {
            "id": "https://quas-modo.github.io/2024/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%86%B3%E7%AD%96%E6%A0%91/",
            "url": "https://quas-modo.github.io/2024/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%86%B3%E7%AD%96%E6%A0%91/",
            "title": "决策树",
            "date_published": "2024-04-01T15:40:56.000Z",
            "content_html": "<p>根据西瓜书《决策树》章节和李沐 b 站相关视频整理。</p>\n<p><span id=\"more\"></span></p>\n<h1 id=\"决策树\"><a class=\"anchor\" href=\"#决策树\">#</a> 决策树</h1>\n<h2 id=\"西瓜书\"><a class=\"anchor\" href=\"#西瓜书\">#</a> 西瓜书</h2>\n<h3 id=\"基本流程\"><a class=\"anchor\" href=\"#基本流程\">#</a> 基本流程</h3>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234738414.png\" alt=\"image-20240401234738414\" /></p>\n<p>什么情况下不需要继续往下分了？三种停止递归条件如下：</p>\n<ol>\n<li>已全为同一种类别 -&gt; 叶节点</li>\n<li>已没有更多的属性往下分（西瓜色泽、敲声、根蒂，无其他属性）-&gt; 谁多当做谁</li>\n<li>遇到了未出现的属性值 -&gt; 父节点谁多当做谁</li>\n</ol>\n<p>最重要的是找到最优的划分属性</p>\n<blockquote>\n<p>名词解释</p>\n<p>先验分布：基于过去研究、专家经验等</p>\n<p>后验分布：先验分布 + 观测数据</p>\n</blockquote>\n<h3 id=\"划分选择\"><a class=\"anchor\" href=\"#划分选择\">#</a> 划分选择</h3>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234759543.png\" alt=\"image-20240401234759543\" /></p>\n<p>分散的类别越多、概率越小，信息熵越大</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234808225.png\" alt=\"image-20240401234808225\" /></p>\n<p>信息增益越大，意味着使用属性 a 来进行划分所获得的纯度提升越大。每一次选取能够获得最大信息增益的属性进行划分，直到不能划分为止。</p>\n<h4 id=\"增益率\"><a class=\"anchor\" href=\"#增益率\">#</a> 增益率</h4>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234827256.png\" alt=\"image-20240401234827256\" /></p>\n<p>C4.5 决策树算法</p>\n<p>增益越大越好，属性可选择值（分支数量）越小越好。</p>\n<blockquote>\n<p>规范化：把不可直接比较的东西，变得可以比较。</p>\n<p>归一化：规范化到 0-1 之间。</p>\n</blockquote>\n<h4 id=\"基尼指数\"><a class=\"anchor\" href=\"#基尼指数\">#</a> 基尼指数</h4>\n<p>CART 决策树学习算法</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234840889.png\" alt=\"image-20240401234840889\" /></p>\n<h3 id=\"剪枝处理\"><a class=\"anchor\" href=\"#剪枝处理\">#</a> 剪枝处理</h3>\n<p>剪枝是决策树对付 “过拟合” 的主要手段！</p>\n<blockquote>\n<p>过拟合：学习了数据集中不该学习的特性，学多了；不是一般规律，而是少量样本有的特性</p>\n</blockquote>\n<h4 id=\"预剪枝\"><a class=\"anchor\" href=\"#预剪枝\">#</a> 预剪枝</h4>\n<p>提前终止某些分支的生长。</p>\n<p>在生成树的时候就进行评估。如果划分后，精度增加，就划分；若精度无提升，就剪枝。</p>\n<p>但在有些情况下，后续划分可能带来性能显著提高，基于 “贪心” 的预剪枝可能会造成欠拟合。</p>\n<h4 id=\"后剪枝\"><a class=\"anchor\" href=\"#后剪枝\">#</a> 后剪枝</h4>\n<p>生成一颗完全树，再 “回头 “剪枝</p>\n<p>后剪枝可以减少欠拟合的风险，但是训练开销较大。</p>\n<h3 id=\"连续与缺失值\"><a class=\"anchor\" href=\"#连续与缺失值\">#</a> 连续与缺失值</h3>\n<h4 id=\"连续值处理\"><a class=\"anchor\" href=\"#连续值处理\">#</a> 连续值处理</h4>\n<p>最简单的处理策略是二分法。</p>\n<p>候选点集是连续属性之间的中位点，挑选增益率最高的二分点。</p>\n<h4 id=\"缺失值处理\"><a class=\"anchor\" href=\"#缺失值处理\">#</a> 缺失值处理</h4>\n<p>仅使用无缺失的样例，是对数据的极大浪费。</p>\n<p>那么如何使用缺失值？基本思路：<strong>样本赋权、权重划分</strong></p>\n<ol>\n<li>如何划分属性选择？</li>\n<li>给定划分属性，若样本在该属性上的值确实，如何进行划分？</li>\n</ol>\n<p>解决方案、思路：</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234853075.png\" alt=\"image-20240401234853075\" /></p>\n<p>回答问题一：如何划分属性选择？</p>\n<p>信息熵的计算增加权重，通过属性权重（非缺失样本概率）与子集的增益率，权衡选择。</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234907588.png\" alt=\"image-20240401234907588\" /></p>\n<p>回答问题二：给定划分属性，若样本在该属性上的值确实，如何进行划分？</p>\n<p>将已知的后验概率作为未知的先验概率进行处理，<strong>缺失值样本将同时进入三个分支</strong>，但在进入分支时，以权重为概率进行划分。</p>\n<h3 id=\"多变量决策树\"><a class=\"anchor\" href=\"#多变量决策树\">#</a> 多变量决策树</h3>\n<p>每个节点的决策不再依赖于单一属性，而是找到合适的线性分类器。</p>\n<h2 id=\"李沐\"><a class=\"anchor\" href=\"#李沐\">#</a> 李沐</h2>\n<p><strong>两个例子</strong>：</p>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234932374.png\" alt=\"image-20240401234932374\" /></p>\n<ul>\n<li>怎么用决策树来做分类：拿一个数据，看一下这个数据在节点上是应该往哪个方向走，一直往下走，走到叶子节点，就会得到数据所对应的标号</li>\n<li>用决策树做回归：与上面那个例子不同的是，叶节点不是一个具体的类而是一个实数值</li>\n</ul>\n<h3 id=\"决策树的好与坏\"><a class=\"anchor\" href=\"#决策树的好与坏\">#</a> <strong>决策树的好与坏</strong></h3>\n<p>好处：</p>\n<ul>\n<li>可以解释（可以让人看到对数据处理的过程）【常用于银行业保险业】；</li>\n<li>可以处理数值类和类别类的特征；</li>\n</ul>\n<p>坏处：</p>\n<ul>\n<li>不稳定（数据产生一定的噪音之后，整棵树构建出的样子可能会不一样）【使用集成学习 (ensemble learning) 可以解决】</li>\n<li>数据过于复杂会生成过于复杂的树，会导致过拟合【把决策树的枝剪掉一些（在训练时觉得太复杂了就停下来，或在训练之后把特往下的节点给剪掉）】</li>\n<li>大量的判断语句（太顺序化），不太好并行【在性能上会吃亏】</li>\n</ul>\n<h3 id=\"让决策树稳定的方法\"><a class=\"anchor\" href=\"#让决策树稳定的方法\">#</a> <strong>让决策树稳定的方法</strong></h3>\n<h4 id=\"随机森林\"><a class=\"anchor\" href=\"#随机森林\">#</a> 随机森林</h4>\n<ul>\n<li>训练多个决策树来提升稳定性：</li>\n<li>每棵树会独立的进行训练，训练之后这些树一起作用得出结果；</li>\n<li>分类的话，可以用投票（少数服从多数）；</li>\n<li>回归的话，实数值可以时每棵树的结果求平均；</li>\n<li>随机来自以下两种情况：\n<ul>\n<li>Bagging：在训练集中随机采样一些样本出来（放回，可重复）；</li>\n<li>在 bagging 出来的数字中，再随机采样一些特征出来，不用整个特征；</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"boosting\"><a class=\"anchor\" href=\"#boosting\">#</a> <strong>Boosting</strong></h4>\n<p><img data-src=\"https://quasdo.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234948093.png\" alt=\"image-20240401234948093\" /></p>\n<ul>\n<li>顺序完成多个树的训练（之前是独立的完成）</li>\n<li>例子说的是，利用训练好的树与真实值做残差来训练新的树，训练好了之后再与之前的树相加</li>\n<li>残差 等价于 取了一个平均均方误差（预测值与真实值的）再求梯度乘上个负号</li>\n</ul>\n<h4 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> <strong>总结：</strong></h4>\n<p>​     树模型在工业界用的比较多【简单，训练算法简单，没有太多的超参数，结果还不错】（不用调参结果还不错）</p>\n<p>​      树模型能够用的时候，通常是第一选择。</p>\n",
            "tags": [
                "nlp",
                "机器学习基础",
                "AI",
                "机器学习"
            ]
        }
    ]
}